{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import visdom\n",
    "import pickle\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import multioutput\n",
    "from sklearn import neural_network\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "from numba import njit, prange, jit\n",
    "\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Novel_Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, active_col, global_mean, base=\"GRU\", filtered=True, ratio_cal=False):\n",
    "        super(Novel_Model,self).__init__()\n",
    "        \n",
    "        active_size = len(active_col)\n",
    "        decayed_size = input_size - active_size\n",
    "        decayed_col = [x for x in range(inputs.shape[-1]) if x not in active_col]\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.base = base\n",
    "        self.filtered = filtered\n",
    "        self.ratio_cal = ratio_cal\n",
    "        \n",
    "        if base==\"GRU\":\n",
    "            self.gru = nn.GRUCell(input_size + input_size, hidden_size, bias = True)\n",
    "        elif base==\"LSTM\":\n",
    "            self.lstm = nn.LSTMCell(input_size + input_size, hidden_size, bias = True)\n",
    "            \n",
    "        if (input_size == len(active_col)) or (len(active_col)==0):\n",
    "            self.is_together = True\n",
    "        else:\n",
    "            self.is_together = False\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size,output_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        if self.is_together:\n",
    "            self.gamma_x_l = nn.Linear(input_size, input_size)\n",
    "            self.ratio1 = nn.Linear(input_size + input_size, input_size)\n",
    "            self.ratio2 = nn.Linear(input_size, input_size)\n",
    "            self.gamma_x_weight = Variable(torch.ones(input_size))     \n",
    "            self.gamma_x_sig = nn.Linear(input_size, input_size)\n",
    "            global_mean = np.array(global_mean)        \n",
    "            self.global_mean = Variable(torch.tensor(global_mean).type(torch.FloatTensor))\n",
    "        else:           \n",
    "            self.gamma_x_l_a = nn.Linear(active_size, active_size)\n",
    "            self.gamma_x_l_d = nn.Linear(decayed_size, decayed_size)\n",
    "            self.gamma_x_sig = nn.Linear(active_size, active_size)\n",
    "            self.gamma_x_weight = Variable(torch.ones(active_size))        \n",
    "            self.zeros_d = Variable(torch.zeros(decayed_size))\n",
    "            global_mean = np.array(global_mean)        \n",
    "            self.global_mean = Variable(torch.tensor(global_mean[decayed_col]).type(torch.FloatTensor))\n",
    "        \n",
    "        self.gamma_h_l = nn.Linear(hidden_size, hidden_size)        \n",
    "            \n",
    "        self.zeros = Variable(torch.zeros(input_size))\n",
    "        \n",
    "        if self.filtered:\n",
    "            self.cong_filter = nn.Linear(hidden_size, hidden_size)\n",
    "            self.free_filter = nn.Linear(hidden_size, hidden_size)\n",
    "        self.filter = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        \n",
    "        self.active_col = active_col\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        hidden = Variable(torch.ones(batch_size,self.hidden_size))\n",
    "        if self.base==\"GRU\":\n",
    "            return hidden\n",
    "        elif self.base == \"LSTM\":\n",
    "            cell = Variable(torch.zeros(batch_size,self.hidden_size))\n",
    "            return hidden, cell\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self,inputs):        \n",
    "        step_size = inputs.size(2)\n",
    "        \n",
    "        active_col = self.active_col\n",
    "        decayed_col = [x for x in range(inputs.shape[-1]) if x not in active_col]\n",
    "        \n",
    "        X = torch.squeeze(inputs[:,0,:,:], dim = 1)       \n",
    "        X_last_obsv = torch.squeeze(inputs[:,1,:,:], dim = 1)        \n",
    "        Mask = torch.squeeze(inputs[:,2,:,:], dim = 1)\n",
    "        Delta = torch.squeeze(inputs[:,3,:,:], dim = 1)\n",
    "        x_imputed = torch.squeeze(inputs[:,4,:,:], dim = 1)\n",
    "        \n",
    "        if self.is_together:\n",
    "            pass\n",
    "        else:\n",
    "            im_decayed = x_imputed[:,:,decayed_col]\n",
    "            im_active = x_imputed[:,:,active_col]\n",
    "\n",
    "            in_decayed = X[:,:,decayed_col]\n",
    "            in_active = X[:,:,active_col]\n",
    "\n",
    "            lo_decayed = X_last_obsv[:,:,decayed_col]\n",
    "            lo_active = X_last_obsv[:,:,active_col]\n",
    "\n",
    "            ma_decayed = Mask[:,:,decayed_col]\n",
    "            ma_active = Mask[:,:,active_col]\n",
    "\n",
    "            dt_decayed = Delta[:,:,decayed_col]\n",
    "            dt_active = Delta[:,:,active_col]\n",
    "        \n",
    "        if self.filtered:\n",
    "            h_filter = torch.squeeze(inputs[:,5,:,:], dim = 1)        \n",
    "        \n",
    "        \n",
    "        if self.base==\"GRU\":\n",
    "            hidden= self.init_hidden(inputs.size(0))        \n",
    "        elif self.base==\"LSTM\":\n",
    "            (hidden, cell) = self.init_hidden(inputs.size(0))\n",
    "        \n",
    "        for i in range(step_size):\n",
    "            \n",
    "            if self.is_together:\n",
    "                x = torch.squeeze(X[:,i:i+1,:], dim = 1)\n",
    "                last_ob = torch.squeeze(X_last_obsv[:,i:i+1,:], dim = 1)\n",
    "                mask = torch.squeeze(Mask[:,i:i+1,:], dim = 1)\n",
    "                delta = torch.squeeze(Delta[:,i:i+1,:], dim = 1)\n",
    "                x_imp = torch.squeeze(x_imputed[:,i:i+1,:], dim = 1) \n",
    "\n",
    "                delta_sig = torch.sigmoid(self.gamma_x_sig(self.gamma_x_weight))\n",
    "                delta_x = torch.exp(-torch.max(self.zeros, self.gamma_x_l(delta)))             \n",
    "                delta_h = torch.exp(-torch.max(self.zeros, self.gamma_h_l(delta)))         \n",
    "\n",
    "                x_a = (delta_sig * last_ob) + ((1-delta_sig) * x_imp)\n",
    "                x_d = mask * x + (1-mask) * (delta_x * last_ob + (1-delta_x) * self.global_mean)                \n",
    "                x_before = torch.cat((x_a, x_d),1)\n",
    "                x1 = self.ratio1(x_before)\n",
    "                x = self.ratio2(x1)            \n",
    "                \n",
    "                if self.filtered:\n",
    "                    h_filter_cong = torch.squeeze(h_filter[:,i:i+1,:], dim=1)\n",
    "                    h_filter_free = torch.ones(h_filter_cong.size())-h_filter_cong\n",
    "\n",
    "                x_input = torch.cat((x, mask),1)\n",
    "            else:\n",
    "\n",
    "                in_d = torch.squeeze(in_decayed[:,i:i+1,:], dim = 1)\n",
    "                in_a = torch.squeeze(in_active[:,i:i+1,:], dim = 1)\n",
    "\n",
    "                lo_d = torch.squeeze(lo_decayed[:,i:i+1,:], dim = 1)\n",
    "                lo_a = torch.squeeze(lo_active[:,i:i+1,:], dim = 1)\n",
    "\n",
    "                ma_d = torch.squeeze(ma_decayed[:,i:i+1,:], dim = 1)\n",
    "                ma_a = torch.squeeze(ma_active[:,i:i+1,:], dim = 1)\n",
    "\n",
    "                dt_d = torch.squeeze(dt_decayed[:,i:i+1,:], dim = 1)\n",
    "                dt_a = torch.squeeze(dt_active[:,i:i+1,:], dim = 1)\n",
    "                dt = torch.cat((dt_a, dt_d), 1)\n",
    "\n",
    "                im_a = torch.squeeze(im_active[:,i:i+1,:], dim = 1) \n",
    "\n",
    "                if self.filtered:\n",
    "                    h_filter_cong = torch.squeeze(h_filter[:,i:i+1,:], dim=1)\n",
    "                    h_filter_free = torch.ones(h_filter_cong.size())-h_filter_cong\n",
    "\n",
    "                delta_sig = torch.sigmoid(self.gamma_x_sig(self.gamma_x_weight))\n",
    "                delta_xa = torch.sigmoid(self.gamma_x_l_a(self.gamma_x_weight))\n",
    "                delta_xd = torch.exp(-torch.max(self.zeros_d, self.gamma_x_l_d(dt_d)))\n",
    "                delta_h = torch.exp(-torch.max(self.zeros, self.gamma_h_l(dt)))\n",
    "                \n",
    "                x_a = ma_a * in_a + (1-ma_a) * (delta_xa * lo_a + (1-delta_xa) * im_a)\n",
    "                x_d = ma_d * in_d + (1-ma_d) * (delta_xd * lo_d + (1-delta_xd) * self.global_mean)\n",
    "                in_concat = torch.cat((x_a,x_d), 1)\n",
    "                ma_concat = torch.cat((ma_a, ma_d), 1)\n",
    "                \n",
    "                x_input = torch.cat((in_concat, ma_concat),1)\n",
    "            \n",
    "            hidden = delta_h * hidden\n",
    "            \n",
    "            if self.base==\"GRU\":\n",
    "                hidden = self.gru(x_input, hidden)\n",
    "            if self.base==\"LSTM\":\n",
    "                (hidden, cell)= self.lstm(x_input, (hidden,cell))\n",
    "                \n",
    "            if self.filtered:            \n",
    "                adj_hidden_cong = self.cong_filter(hidden)\n",
    "                adj_hidden_free = self.free_filter(hidden)\n",
    "                hidden = adj_hidden_cong * h_filter_cong + adj_hidden_free * h_filter_free\n",
    "            \n",
    "            hidden = self.filter(hidden)\n",
    "        \n",
    "        output = self.linear(hidden)\n",
    "        \n",
    "        if self.ratio_cal:\n",
    "            add = (output[:,0]*output[:,2])+(output[:,1]*(1-output[:,2]))\n",
    "            add_col = add.unsqueeze(1)\n",
    "            output_add = torch.cat((output, add_col),1)\n",
    "            return output_add\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset_renew:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 low_speed_matrix, high_speed_matrix, mean_speed_matrix,\n",
    "                 low_count_matrix, high_count_matrix, all_count_matrix,                 \n",
    "                 input_links, label_link,\n",
    "                 thresholds,\n",
    "                 train_proportion = 0.6, valid_proportion = 0.2, time_unit = 3, seed = 1024,\n",
    "                 batch = 1000, seq_length = 7\n",
    "                ):\n",
    "        \n",
    "        \n",
    "        self.input_links = input_links\n",
    "        self.label_link = label_link\n",
    "        self.input_links.append(self.label_link)\n",
    "        self.time_unit = time_unit\n",
    "        \n",
    "        empty_df = pd.DataFrame(index = all_count_matrix.index, columns = all_count_matrix.columns)[self.input_links]\n",
    "                \n",
    "        self.seed = seed\n",
    "        self.train_proportion = train_proportion\n",
    "        self.valid_proportion = valid_proportion\n",
    "        self.batch = batch\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.low_speed = empty_df.copy()\n",
    "        self.low_speed.update(low_speed_matrix)\n",
    "        \n",
    "        self.high_speed = empty_df.copy()\n",
    "        self.high_speed.update(high_speed_matrix)\n",
    "        \n",
    "        self.mean_speed = empty_df.copy()\n",
    "        self.mean_speed.update(mean_speed_matrix)\n",
    "        \n",
    "        self.low_count = empty_df.copy()\n",
    "        self.low_count.update(low_count_matrix)\n",
    "        \n",
    "        self.high_count = empty_df.copy()\n",
    "        self.high_count.update(high_count_matrix)\n",
    "        \n",
    "        self.all_count = empty_df.copy()\n",
    "        self.all_count.update(all_count_matrix)\n",
    "        \n",
    "        self.low_ratio = self.low_count.fillna(0)/self.all_count\n",
    "        self.label_nan_index = self.mean_speed[self.mean_speed[self.label_link].isnull()].index\n",
    "        \n",
    "        self.threshold = thresholds.loc[self.input_links, \"threshold\"].to_numpy()\n",
    "        \n",
    "    def delta_time(self, inputs):\n",
    "        result = inputs.copy()\n",
    "        result = result.isnull().astype(int)\n",
    "        result.iloc[0,:]=0\n",
    "        for i in tqdm_notebook(range(1, result.shape[0]), desc=\"Delta Time\"):\n",
    "            delta = (result.index[i]-result.index[i-1]).total_seconds()/60    \n",
    "            result.iloc[i,:] = result.iloc[i,:]*(delta + result.iloc[i-1,:])            \n",
    "        return result\n",
    "    \n",
    "    def data_cut(self, inputs, name=\"Data Cut\"):\n",
    "        #input: Pandas DF\n",
    "        cutted = np.zeros([inputs.shape[0]-self.seq_length+1, self.seq_length, inputs.shape[1]])\n",
    "        for i in tqdm_notebook(range(cutted.shape[0]), desc=name):\n",
    "            cutted[i] = inputs.iloc[i:i+self.seq_length]\n",
    "        return cutted\n",
    "    \n",
    "    def random_forest(self,inputs):\n",
    "        fit_input = self.data_cut(inputs)\n",
    "        fit_size = fit_input.shape[1]*fit_input.shape[2]\n",
    "        fit_input = fit_input.reshape(-1, fit_size)\n",
    "        fit_df = pd.DataFrame(fit_input)\n",
    "        fit_isnan = np.isnan(fit_input)\n",
    "        fit_isnan_uni = np.unique(fit_isnan, axis=0)\n",
    "        init = np.zeros((fit_input.shape[1],)).astype(bool)\n",
    "        address_init = np.where((fit_isnan==init).all(axis=1))\n",
    "        if address_init[0].shape[0]>10000:   \n",
    "            fit_init = fit_input[address_init[0]]\n",
    "        else:\n",
    "            fit_init = fit_input[address_init]\n",
    "        pbar = tqdm_notebook(range(fit_isnan_uni.shape[0]))\n",
    "        pbar.set_description(\"random forest \")\n",
    "        for isnan in fit_isnan_uni:\n",
    "            if (isnan==init).all():\n",
    "                pass\n",
    "            elif isnan.sum()==fit_size:\n",
    "                pass            \n",
    "            else:           \n",
    "                address = np.where((fit_isnan==isnan).all(axis=1))[0]                \n",
    "                fit_X = fit_init[:,np.where(1-isnan)[0]]\n",
    "                fit_Y = fit_init[:,np.where(isnan)[0]]\n",
    "                if fit_Y.shape[0]==fit_Y.size:\n",
    "                    fit_Y = fit_Y.ravel()\n",
    "                regressor = ensemble.RandomForestRegressor(n_estimators=50, n_jobs=-1) \n",
    "                index = np.random.choice(fit_X.shape[0], int(fit_X.shape[0]*0.4))\n",
    "                regressor.fit(fit_X[index], fit_Y[index])\n",
    "                pred_X = fit_input[:, np.where(1-isnan)[0]][address]\n",
    "                pred_Y = regressor.predict(pred_X)\n",
    "                if pred_Y.shape[0]==pred_Y.size:\n",
    "                    pred_Y = pred_Y.reshape(pred_Y.size,1)\n",
    "                fit_df.loc[address, isnan]=pred_Y\n",
    "            pbar.update()        \n",
    "        \n",
    "        result_1 = pd.DataFrame(index = inputs.index, columns = fit_df.columns)\n",
    "        result_2 = pd.DataFrame(index = inputs.index, columns = inputs.columns)\n",
    "        for i in range(self.seq_length):\n",
    "            result_1.iloc[i:i+fit_df.shape[0],i*inputs.shape[1]:(i+1)*inputs.shape[1]] = fit_df.iloc[:,i*inputs.shape[1]:(i+1)*inputs.shape[1]].to_numpy()\n",
    "\n",
    "        for j in range(result_2.columns.shape[0]):    \n",
    "            result_2.iloc[:,j] = result_1.iloc[:,list(range(j,result_2.columns.shape[0]*self.seq_length,result_2.columns.shape[0]))].mean(axis=1).to_numpy()        \n",
    "        \n",
    "        return result_2\n",
    "    \n",
    "    def moving_average(self,inputs):\n",
    "        result = inputs.copy()\n",
    "        for j in range(result.shape[1]):\n",
    "            for i in range(self.seq_length):\n",
    "                if pd.isnull(result.iloc[i+self.seq_length,j]):\n",
    "                    result.iloc[i,j] = result.iloc[:i,j].mean()\n",
    "                \n",
    "                \n",
    "        for j in range(result.shape[1]):\n",
    "            for i in range(result.shape[0]-self.seq_length):                \n",
    "                if pd.isnull(result.iloc[i+self.seq_length,j]):\n",
    "                    result.iloc[i+self.seq_length,j] = result.iloc[i:i+self.seq_length,j].mean()\n",
    "                    \n",
    "        return result\n",
    "    \n",
    "    def last_observ(self, inputs):\n",
    "        result = inputs.copy()\n",
    "        result.iloc[0,:] = result.iloc[0,:].fillna(0)\n",
    "        for i in tqdm_notebook(range(1, result.shape[0]), desc=\"Last Observed\"):            \n",
    "            result.iloc[i,:] = result.iloc[i-1,:] * result.iloc[i,:].isnull() + result.iloc[i,:].fillna(0)            \n",
    "        return result\n",
    "\n",
    "    def estm_prepare_matrix(self,inputs):\n",
    "        fit_input = self.data_cut(inputs, \"Estimation Prepare\")\n",
    "        size_1d = fit_input.shape[1]\n",
    "        size_2d = fit_input.shape[2]\n",
    "        fit_size = size_1d * size_2d\n",
    "        fit_input = fit_input.reshape(-1, fit_size)\n",
    "        fit_Y = fit_input[:,-1]\n",
    "        fit_X = fit_input[:,0:-1]\n",
    "        regressor = ensemble.RandomForestRegressor(n_estimators=50, n_jobs=-1)                \n",
    "        index = np.random.choice(fit_X.shape[0], int(fit_X.shape[0]*0.6))\n",
    "        regressor.fit(fit_X[index], fit_Y[index])\n",
    "        new_label = regressor.predict(fit_X)\n",
    "        new_input = np.concatenate((fit_X, new_label.reshape(-1,1)),axis=1)\n",
    "        new_input = new_input.reshape(-1, size_1d, size_2d)\n",
    "        print(\"Random Forest MAE: \", np.round(np.abs(new_label-fit_Y).mean(),5))\n",
    "        \n",
    "        return new_input, fit_Y\n",
    "    \n",
    "    \n",
    "    def fore_prepare_matrix(self, inputs):\n",
    "        fit_input = self.data_cut(inputs, \"Forecast Prepare\")\n",
    "        size_1d = fit_input.shape[1]\n",
    "        size_2d = fit_input.shape[2]\n",
    "        fit_size = size_1d * size_2d\n",
    "        fit_input = fit_input.reshape(-1, fit_size)\n",
    "        fit_Y = fit_input[1:,-1]\n",
    "        fit_X = fit_input[:-1,:]         \n",
    "        new_input = fit_X.reshape(-1,size_1d, size_2d)        \n",
    "        return new_input, fit_Y\n",
    "    \n",
    "    def pre_prepare(self, low_spd_rf=None, hig_spd_rf=None, low_rto_rf=None, all_spd_rf=None, result=False, max_speed=120):\n",
    "        self.low_spd = self.low_speed.drop(self.label_nan_index)[self.input_links]\n",
    "        self.hig_spd = self.high_speed.drop(self.label_nan_index)[self.input_links]\n",
    "        self.low_rto = self.low_ratio.drop(self.label_nan_index)[self.input_links]\n",
    "        self.all_spd = self.mean_speed.drop(self.label_nan_index)[self.input_links]\n",
    "        \n",
    "        self.low_spd_im = pd.DataFrame(index = self.all_spd.index, columns = self.all_spd.columns)\n",
    "        self.hig_spd_im = pd.DataFrame(index = self.all_spd.index, columns = self.all_spd.columns)\n",
    "        self.low_rto_im = pd.DataFrame(index = self.all_spd.index, columns = self.all_spd.columns)\n",
    "        self.all_spd_im = pd.DataFrame(index = self.all_spd.index, columns = self.all_spd.columns)\n",
    "        \n",
    "        if low_spd_rf is None:\n",
    "            df = self.random_forest(self.low_spd)\n",
    "            df = self.moving_average(df)\n",
    "            self.low_spd_im.update(df)\n",
    "        else:\n",
    "            self.low_spd_im.update(low_spd_rf[self.input_links])\n",
    "            \n",
    "        if hig_spd_rf is None:\n",
    "            df = self.random_forest(self.hig_spd)\n",
    "            df = self.moving_average(df)\n",
    "            self.low_spd_im.update(df)\n",
    "        else:\n",
    "            self.hig_spd_im.update(hig_spd_rf[self.input_links])\n",
    "            \n",
    "        if low_rto_rf is None:\n",
    "            df = self.random_forest(self.low_rto)\n",
    "            df = self.moving_average(df)\n",
    "            self.low_spd_im.update(df)\n",
    "        else:\n",
    "            self.low_rto_im.update(low_rto_rf[self.input_links])\n",
    "            \n",
    "        if all_spd_rf is None:\n",
    "            df = self.random_forest(self.all_spd)\n",
    "            df = self.moving_average(df)\n",
    "            self.low_spd_im.update(df)\n",
    "        else:\n",
    "            self.all_spd_im.update(all_spd_rf[self.input_links])\n",
    "            \n",
    "        self.low_spd_ma = (~self.low_spd.isnull()).astype(int)\n",
    "        self.hig_spd_ma = (~self.hig_spd.isnull()).astype(int)\n",
    "        self.low_rto_ma = (~self.low_rto.isnull()).astype(int)\n",
    "        self.all_spd_ma = (~self.all_spd.isnull()).astype(int)\n",
    "        \n",
    "        self.filter = (self.all_spd_im>=self.threshold).astype(int)\n",
    "        \n",
    "        self.concat_df = pd.concat((self.low_spd/max_speed, self.hig_spd/max_speed, self.low_rto, self.all_spd/max_speed), axis=1)\n",
    "        self.concat_ma = (~self.concat_df.isnull()).astype(int)\n",
    "        self.concat_lo = self.last_observ(self.concat_df)\n",
    "        self.concat_dt = self.delta_time(self.concat_df)     \n",
    "        self.concat_im = pd.concat((self.low_spd_im/max_speed, self.hig_spd_im/max_speed, self.low_rto_im, self.all_spd_im/max_speed), axis=1)\n",
    "        self.concat_ft = pd.concat((self.filter, self.filter, self.filter, self.filter), axis = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.low_spd_im_st = self.low_spd_im/max_speed\n",
    "        self.hig_spd_im_st = self.hig_spd_im/max_speed        \n",
    "        self.all_spd_im_st = self.all_spd_im/max_speed\n",
    "        \n",
    "        each = int(self.concat_df.shape[1]/4)\n",
    "        \n",
    "        self.low_spd_lo = self.concat_lo.iloc[:,0*each:1*each]\n",
    "        self.hig_spd_lo = self.concat_lo.iloc[:,1*each:2*each]\n",
    "        self.low_rto_lo = self.concat_lo.iloc[:,2*each:3*each]\n",
    "        self.all_spd_lo = self.concat_lo.iloc[:,3*each:4*each]        \n",
    "        \n",
    "        self.low_spd_dt = self.concat_dt.iloc[:,0*each:1*each]\n",
    "        self.hig_spd_dt = self.concat_dt.iloc[:,1*each:2*each]\n",
    "        self.low_rto_dt = self.concat_dt.iloc[:,2*each:3*each]\n",
    "        self.all_spd_dt = self.concat_dt.iloc[:,3*each:4*each]\n",
    "        \n",
    "        self.global_mean = self.concat_df.mean()\n",
    "        \n",
    "    def make_dataloader_estm(self):\n",
    "        self.data_input = np.nan_to_num(self.data_cut(self.concat_df, \"Data Input\"))\n",
    "        self.mask_input = self.data_cut(self.concat_ma, \"Mask Input\")\n",
    "        self.laob_input = self.data_cut(self.concat_lo, \"Last Observation\")\n",
    "        self.delt_input = self.data_cut(self.concat_dt, \"Delta Time\")      \n",
    "        self.filt_input = self.data_cut(self.concat_ft, \"Filter\")\n",
    "        \n",
    "        \n",
    "        #label data input에서 제거\n",
    "        '''\n",
    "        self.data_input[:, self.data_input.shape[1]-1, self.data_input.shape[2]-1] = 0\n",
    "        self.mask_input[:, self.data_input.shape[1]-1, self.data_input.shape[2]-1] = 0\n",
    "        self.laob_input[:, self.data_input.shape[1]-1, self.data_input.shape[2]-1] = self.laob_input[:, self.data_input.shape[1]-2, self.data_input.shape[2]-1]\n",
    "        self.delt_input[:, self.data_input.shape[1]-1, self.data_input.shape[2]-1] = self.delt_input[:, self.data_input.shape[1]-2, self.data_input.shape[2]-1]+self.time_unit        \n",
    "        '''\n",
    "        #2,5,8,11\n",
    "        indexing = len(self.input_links)\n",
    "        index_list = [indexing-1, 2*indexing-1, 3*indexing-1, 4*indexing-1]\n",
    "        \n",
    "        self.data_input[:, self.data_input.shape[1]-1, index_list] = 0\n",
    "        self.mask_input[:, self.data_input.shape[1]-1, index_list] = 0\n",
    "        self.laob_input[:, self.data_input.shape[1]-1, index_list] = self.laob_input[:, self.data_input.shape[1]-2, index_list]\n",
    "        self.delt_input[:, self.data_input.shape[1]-1, index_list] = self.delt_input[:, self.data_input.shape[1]-2, index_list]+self.time_unit        \n",
    "        \n",
    "        self.impu_spd_low, self.spdlw_label = self.estm_prepare_matrix(self.low_spd_im_st)\n",
    "        self.impu_spd_hig, self.spdhi_label = self.estm_prepare_matrix(self.hig_spd_im_st)\n",
    "        self.impu_low_rto, self.lowrt_label = self.estm_prepare_matrix(self.low_rto_im)\n",
    "        self.impu_spd_all, self.speed_label = self.estm_prepare_matrix(self.all_spd_im_st)\n",
    "        \n",
    "        self.impu_input = np.concatenate((self.impu_spd_low, self.impu_spd_hig, self.impu_low_rto, self.impu_spd_all), axis=2)       \n",
    "        \n",
    "        \n",
    "        sample_size = self.data_input.shape[0]\n",
    "        train_index = int(np.floor(sample_size * self.train_proportion))\n",
    "        valid_index = int(np.floor(sample_size * (self.train_proportion + self.valid_proportion)))\n",
    "        \n",
    "        \n",
    "        train_i = self.concat_df.index[self.seq_length-1:train_index+self.seq_length-1]\n",
    "        valid_i = self.concat_df.index[train_index+self.seq_length-1:valid_index+self.seq_length-1]\n",
    "        test_i = self.concat_df.index[valid_index+self.seq_length-1:]\n",
    "        \n",
    "        train_size = int(np.floor(train_i.size/self.batch)*self.batch)\n",
    "        valid_size = int(np.floor(valid_i.size/self.batch)*self.batch)\n",
    "        test_size = int(np.floor((test_i.size-self.seq_length+1)/self.batch)*self.batch)\n",
    "        \n",
    "        train_i = train_i[:train_size]\n",
    "        valid_i = valid_i[:valid_size]\n",
    "        test_i = test_i[:test_size]\n",
    "        \n",
    "        data_branch = np.expand_dims(self.data_input, axis = 1)\n",
    "        mask_branch = np.expand_dims(self.mask_input, axis = 1)\n",
    "        laob_branch = np.expand_dims(self.laob_input, axis = 1)\n",
    "        delt_branch = np.expand_dims(self.delt_input, axis = 1)\n",
    "        impu_branch = np.expand_dims(self.impu_input, axis = 1)\n",
    "        filt_branch = np.expand_dims(self.filt_input, axis = 1)\n",
    "        \n",
    "        total_dataset = np.concatenate((data_branch, mask_branch, laob_branch, delt_branch, impu_branch, filt_branch), axis=1)\n",
    "        \n",
    "        train_data, train_label = total_dataset[:train_index], self.speed_label[:train_index]\n",
    "        valid_data, valid_label = total_dataset[train_index:valid_index], self.speed_label[train_index:valid_index]\n",
    "        test_data, test_label = total_dataset[valid_index:], self.speed_label[valid_index:]\n",
    "\n",
    "        train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "        valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
    "        test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "\n",
    "        train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "        valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
    "        test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "\n",
    "        train_dataloader = utils.DataLoader(train_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        valid_dataloader = utils.DataLoader(valid_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        test_dataloader = utils.DataLoader(test_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        \n",
    "        return train_dataloader, valid_dataloader, test_dataloader, train_i, valid_i, test_i\n",
    "        \n",
    "    \n",
    "    def make_dataloader_fore(self):\n",
    "        self.data_input = np.nan_to_num(self.data_cut(self.concat_df, \"Data Input\"))[:-1]\n",
    "        self.mask_input = self.data_cut(self.concat_ma, \"Mask Input\")[:-1]\n",
    "        self.laob_input = self.data_cut(self.concat_lo, \"Last Observation\")[:-1]\n",
    "        self.delt_input = self.data_cut(self.concat_dt, \"Delta Time\")[:-1]\n",
    "        self.filt_input = self.data_cut(self.concat_ft, \"Filter\")[:-1]\n",
    "        \n",
    "        self.impu_spd_low, self.spdlw_label = self.fore_prepare_matrix(self.low_spd_im_st)\n",
    "        self.impu_spd_hig, self.spdhi_label = self.fore_prepare_matrix(self.hig_spd_im_st)\n",
    "        self.impu_low_rto, self.lowrt_label = self.fore_prepare_matrix(self.low_rto_im)\n",
    "        self.impu_spd_all, self.speed_label = self.fore_prepare_matrix(self.all_spd_im_st)\n",
    "        \n",
    "        self.impu_input = np.concatenate((self.impu_spd_low, self.impu_spd_hig, self.impu_low_rto, self.impu_spd_all), axis=2)\n",
    "        \n",
    "        self.global_mean = self.concat_df.mean()\n",
    "        \n",
    "        sample_size = self.data_input.shape[0]\n",
    "        train_index = int(np.floor(sample_size * self.train_proportion))\n",
    "        valid_index = int(np.floor(sample_size * (self.train_proportion + self.valid_proportion)))\n",
    "        \n",
    "        train_i = self.concat_df.index[:-1][self.seq_length-1:train_index+self.seq_length-1]\n",
    "        valid_i = self.concat_df.index[:-1][train_index+self.seq_length-1:valid_index+self.seq_length-1]\n",
    "        test_i = self.concat_df.index[:-1][valid_index+self.seq_length-1:]\n",
    "        \n",
    "        train_size = int(np.floor(train_i.size/self.batch)*self.batch)\n",
    "        valid_size = int(np.floor(valid_i.size/self.batch)*self.batch)\n",
    "        test_size = int(np.floor((test_i.size-self.seq_length+1)/self.batch)*self.batch)\n",
    "        \n",
    "        train_i = train_i[:train_size]\n",
    "        valid_i = valid_i[:valid_size]\n",
    "        test_i = test_i[:test_size]\n",
    "        \n",
    "        data_branch = np.expand_dims(self.data_input, axis = 1)\n",
    "        mask_branch = np.expand_dims(self.mask_input, axis = 1)\n",
    "        laob_branch = np.expand_dims(self.laob_input, axis = 1)\n",
    "        delt_branch = np.expand_dims(self.delt_input, axis = 1)\n",
    "        impu_branch = np.expand_dims(self.impu_input, axis = 1)\n",
    "        filt_branch = np.expand_dims(self.filt_input, axis = 1)\n",
    "        \n",
    "        total_dataset = np.concatenate((data_branch, mask_branch, laob_branch, delt_branch, impu_branch, filt_branch), axis=1)\n",
    "        \n",
    "        train_data, train_label = total_dataset[:train_index], self.speed_label[:train_index]\n",
    "        valid_data, valid_label = total_dataset[train_index:valid_index], self.speed_label[train_index:valid_index]\n",
    "        test_data, test_label = total_dataset[valid_index:], self.speed_label[valid_index:]\n",
    "\n",
    "        train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "        valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
    "        test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "\n",
    "        train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "        valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
    "        test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "\n",
    "        train_dataloader = utils.DataLoader(train_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        valid_dataloader = utils.DataLoader(valid_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        test_dataloader = utils.DataLoader(test_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        \n",
    "        return train_dataloader, valid_dataloader, test_dataloader, train_i, valid_i, test_i\n",
    "    \n",
    "    \n",
    "   \n",
    "    def one_step_prepare_matrix(self, inputs):\n",
    "        new_input = inputs.loc[:,self.label_link].iloc[self.seq_length-1:,:-1].to_numpy().astype(float)\n",
    "        new_label = inputs.loc[:,self.label_link].iloc[self.seq_length-1:,-1].to_numpy().astype(float)\n",
    "        \n",
    "        return new_input, new_label\n",
    "    \n",
    "    def make_dataloader_one_step(self):\n",
    "        \n",
    "        total_dataset, self.speed_label = self.one_step_prepare_matrix(self.concat_im)\n",
    "        \n",
    "        sample_size = total_dataset.shape[0]\n",
    "        train_index = int(np.floor(sample_size * self.train_proportion))\n",
    "        valid_index = int(np.floor(sample_size * (self.train_proportion + self.valid_proportion)))\n",
    "        \n",
    "        train_data, train_label = total_dataset[:train_index], self.speed_label[:train_index]\n",
    "        valid_data, valid_label = total_dataset[train_index:valid_index], self.speed_label[train_index:valid_index]\n",
    "        test_data, test_label = total_dataset[valid_index:], self.speed_label[valid_index:]\n",
    "        \n",
    "        train_i = self.concat_im.index[:train_index]\n",
    "        valid_i = self.concat_im.index[train_index:valid_index]\n",
    "        test_i = self.concat_im.index[:valid_index:]\n",
    "        \n",
    "        train_size = int(np.floor(train_i.size/self.batch)*self.batch)\n",
    "        valid_size = int(np.floor(valid_i.size/self.batch)*self.batch)\n",
    "        test_size = int(np.floor(test_i.size/self.batch)*self.batch)\n",
    "        \n",
    "        train_i = train_i[:train_size]\n",
    "        valid_i = valid_i[:valid_size]\n",
    "        test_i = test_i[:test_size]\n",
    "\n",
    "        train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "        valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
    "        test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "\n",
    "        train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "        valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
    "        test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "\n",
    "        train_dataloader = utils.DataLoader(train_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        valid_dataloader = utils.DataLoader(valid_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        test_dataloader = utils.DataLoader(test_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        \n",
    "        return train_dataloader, valid_dataloader, test_dataloader, train_i, valid_i, test_i\n",
    "    \n",
    "    def two_step_prepare_matrix(self,inputs):\n",
    "        inputs = inputs.iloc[:,:-len(self.input_links)]\n",
    "        fit_input = self.data_cut(inputs, \"Two Step Prepare\")        \n",
    "        size_1d = fit_input.shape[1]\n",
    "        size_2d = fit_input.shape[2]\n",
    "        fit_size = size_1d * size_2d\n",
    "        fit_input = fit_input.reshape(-1, fit_size)\n",
    "        Y_index = [fit_size-int(size_2d/3)*2-1, fit_size-int(size_2d/3)-1, fit_size-1]\n",
    "        X_index = [x for x in range(fit_size) if x not in Y_index]        \n",
    "        fit_Y = fit_input[:,Y_index]\n",
    "        fit_X = fit_input[:,X_index]\n",
    "        regressor = ensemble.RandomForestRegressor(n_estimators=50, n_jobs=-1)                \n",
    "        index = np.random.choice(fit_X.shape[0], int(fit_X.shape[0]*0.6))\n",
    "        regressor.fit(fit_X[index], fit_Y[index])\n",
    "        new_label = regressor.predict(fit_X)\n",
    "        new_input = fit_input.copy()\n",
    "        new_input[:,Y_index] = new_label        \n",
    "        #new_input = np.concatenate((fit_X, new_label.reshape(-1,3),axis=1)        \n",
    "        new_input = new_input.reshape(-1, size_1d, size_2d)\n",
    "        print(\"Random Forest MAE: \", np.round(np.abs(new_label-fit_Y).mean(),5))\n",
    "        \n",
    "        return new_input, fit_Y\n",
    "    \n",
    "    def make_dataloader_two_step(self):\n",
    "        self.data_twost = np.nan_to_num(self.data_cut(self.concat_df.iloc[:,:-len(self.input_links)], \"Data Input\"))\n",
    "        self.mask_twost = self.data_cut(self.concat_ma.iloc[:,:-len(self.input_links)], \"Mask Input\")\n",
    "        self.laob_twost = self.data_cut(self.concat_lo.iloc[:,:-len(self.input_links)], \"Last Observation\")\n",
    "        self.delt_twost = self.data_cut(self.concat_dt.iloc[:,:-len(self.input_links)], \"Delta Time\")\n",
    "        self.filt_twost = self.data_cut(self.concat_ft.iloc[:,:-len(self.input_links)], \"Filter\")\n",
    "        \n",
    "        self.impu_twost, self.concat_labels = self.two_step_prepare_matrix(self.concat_im)\n",
    "        self.mean_twost = self.concat_df.iloc[:,:-len(self.input_links)].mean()\n",
    "        \n",
    "        #2,5,8\n",
    "        indexing = len(self.input_links)\n",
    "        index_list = [indexing-1, 2*indexing-1, 3*indexing-1]\n",
    "        \n",
    "        self.data_twost[:, self.data_twost.shape[1]-1, index_list] = 0\n",
    "        self.mask_twost[:, self.data_twost.shape[1]-1, index_list] = 0\n",
    "        self.laob_twost[:, self.data_twost.shape[1]-1, index_list] = self.laob_twost[:, self.data_twost.shape[1]-2, index_list]\n",
    "        self.delt_twost[:, self.data_twost.shape[1]-1, index_list] = self.delt_twost[:, self.data_twost.shape[1]-2, index_list]+self.time_unit  \n",
    "        \n",
    "        sample_size = self.data_twost.shape[0]\n",
    "        train_index = int(np.floor(sample_size * self.train_proportion))\n",
    "        valid_index = int(np.floor(sample_size * (self.train_proportion + self.valid_proportion)))\n",
    "        \n",
    "        train_i = self.concat_df.index[:-1][self.seq_length-1:train_index+self.seq_length-1]\n",
    "        valid_i = self.concat_df.index[:-1][train_index+self.seq_length-1:valid_index+self.seq_length-1]\n",
    "        test_i = self.concat_df.index[:-1][valid_index+self.seq_length-1:]\n",
    "        \n",
    "        train_size = int(np.floor(train_i.size/self.batch)*self.batch)\n",
    "        valid_size = int(np.floor(valid_i.size/self.batch)*self.batch)\n",
    "        test_size = int(np.floor((test_i.size-self.seq_length+1)/self.batch)*self.batch)\n",
    "        \n",
    "        train_i = train_i[:train_size]\n",
    "        valid_i = valid_i[:valid_size]\n",
    "        test_i = test_i[:test_size]\n",
    "        \n",
    "        data_branch = np.expand_dims(self.data_twost, axis = 1)\n",
    "        mask_branch = np.expand_dims(self.mask_twost, axis = 1)\n",
    "        laob_branch = np.expand_dims(self.laob_twost, axis = 1)\n",
    "        delt_branch = np.expand_dims(self.delt_twost, axis = 1)\n",
    "        impu_branch = np.expand_dims(self.impu_twost, axis = 1)\n",
    "        filt_branch = np.expand_dims(self.filt_twost, axis = 1)\n",
    "        \n",
    "        total_dataset = np.concatenate((data_branch, mask_branch, laob_branch, delt_branch, impu_branch, filt_branch), axis=1)\n",
    "        \n",
    "        train_data, train_label = total_dataset[:train_index], self.concat_labels[:train_index]\n",
    "        valid_data, valid_label = total_dataset[train_index:valid_index], self.concat_labels[train_index:valid_index]\n",
    "        test_data, test_label = total_dataset[valid_index:], self.concat_labels[valid_index:]\n",
    "\n",
    "        train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "        valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
    "        test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "\n",
    "        train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "        valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
    "        test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "\n",
    "        train_dataloader = utils.DataLoader(train_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        valid_dataloader = utils.DataLoader(valid_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        test_dataloader = utils.DataLoader(test_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        \n",
    "        return train_dataloader, valid_dataloader, test_dataloader, train_i, valid_i, test_i\n",
    "    \n",
    "    def calcul_prepare_matrix(self,inputs):        \n",
    "        fit_input = self.data_cut(inputs, \"Two Step Prepare\")        \n",
    "        size_1d = fit_input.shape[1]\n",
    "        size_2d = fit_input.shape[2]\n",
    "        fit_size = size_1d * size_2d\n",
    "        fit_input = fit_input.reshape(-1, fit_size)\n",
    "        Y_index = [fit_size-int(size_2d/4)*3-1, fit_size-int(size_2d/4)*2-1, fit_size-int(size_2d/4)-1, fit_size-1]\n",
    "        X_index = [x for x in range(fit_size) if x not in Y_index]        \n",
    "        fit_Y = fit_input[:,Y_index]\n",
    "        fit_X = fit_input[:,X_index]\n",
    "        regressor = ensemble.RandomForestRegressor(n_estimators=50, n_jobs=-1)                \n",
    "        index = np.random.choice(fit_X.shape[0], int(fit_X.shape[0]*0.6))\n",
    "        regressor.fit(fit_X[index], fit_Y[index])\n",
    "        new_label = regressor.predict(fit_X)\n",
    "        new_input = fit_input.copy()\n",
    "        new_input[:,Y_index] = new_label        \n",
    "        #new_input = np.concatenate((fit_X, new_label.reshape(-1,3),axis=1)        \n",
    "        new_input = new_input.reshape(-1, size_1d, size_2d)\n",
    "        print(\"Random Forest MAE: \", np.round(np.abs(new_label-fit_Y).mean(),5))\n",
    "        \n",
    "        return new_input, fit_Y\n",
    "    \n",
    "    def make_dataloader_calcul(self):\n",
    "        self.data_twost = np.nan_to_num(self.data_cut(self.concat_df.iloc[:,:-len(self.input_links)], \"Data Input\"))\n",
    "        self.mask_twost = self.data_cut(self.concat_ma.iloc[:,:-len(self.input_links)], \"Mask Input\")\n",
    "        self.laob_twost = self.data_cut(self.concat_lo.iloc[:,:-len(self.input_links)], \"Last Observation\")\n",
    "        self.delt_twost = self.data_cut(self.concat_dt.iloc[:,:-len(self.input_links)], \"Delta Time\")\n",
    "        self.filt_twost = self.data_cut(self.concat_ft.iloc[:,:-len(self.input_links)], \"Filter\")\n",
    "        \n",
    "        self.impu_twost, self.concat_labels = self.calcul_prepare_matrix(self.concat_im)\n",
    "        self.mean_twost = self.concat_df.iloc[:,:-len(self.input_links)].mean()\n",
    "        \n",
    "        #2,5,8\n",
    "        indexing = len(self.input_links)\n",
    "        index_list = [indexing-1, 2*indexing-1, 3*indexing-1]\n",
    "        \n",
    "        self.data_twost[:, self.data_twost.shape[1]-1, index_list] = 0\n",
    "        self.mask_twost[:, self.data_twost.shape[1]-1, index_list] = 0\n",
    "        self.laob_twost[:, self.data_twost.shape[1]-1, index_list] = self.laob_twost[:, self.data_twost.shape[1]-2, index_list]\n",
    "        self.delt_twost[:, self.data_twost.shape[1]-1, index_list] = self.delt_twost[:, self.data_twost.shape[1]-2, index_list]+self.time_unit  \n",
    "        \n",
    "        sample_size = self.data_twost.shape[0]\n",
    "        train_index = int(np.floor(sample_size * self.train_proportion))\n",
    "        valid_index = int(np.floor(sample_size * (self.train_proportion + self.valid_proportion)))\n",
    "        \n",
    "        train_i = self.concat_df.index[:-1][self.seq_length-1:train_index+self.seq_length-1]\n",
    "        valid_i = self.concat_df.index[:-1][train_index+self.seq_length-1:valid_index+self.seq_length-1]\n",
    "        test_i = self.concat_df.index[:-1][valid_index+self.seq_length-1:]\n",
    "        \n",
    "        train_size = int(np.floor(train_i.size/self.batch)*self.batch)\n",
    "        valid_size = int(np.floor(valid_i.size/self.batch)*self.batch)\n",
    "        test_size = int(np.floor((test_i.size-self.seq_length+1)/self.batch)*self.batch)\n",
    "        \n",
    "        train_i = train_i[:train_size]\n",
    "        valid_i = valid_i[:valid_size]\n",
    "        test_i = test_i[:test_size]\n",
    "        \n",
    "        data_branch = np.expand_dims(self.data_twost, axis = 1)\n",
    "        mask_branch = np.expand_dims(self.mask_twost, axis = 1)\n",
    "        laob_branch = np.expand_dims(self.laob_twost, axis = 1)\n",
    "        delt_branch = np.expand_dims(self.delt_twost, axis = 1)\n",
    "        impu_branch = np.expand_dims(self.impu_twost[:,:,:-len(self.input_links)], axis = 1)\n",
    "        filt_branch = np.expand_dims(self.filt_twost, axis = 1)\n",
    "        \n",
    "        total_dataset = np.concatenate((data_branch, mask_branch, laob_branch, delt_branch, impu_branch, filt_branch), axis=1)\n",
    "        \n",
    "        train_data, train_label = total_dataset[:train_index], self.concat_labels[:train_index]\n",
    "        valid_data, valid_label = total_dataset[train_index:valid_index], self.concat_labels[train_index:valid_index]\n",
    "        test_data, test_label = total_dataset[valid_index:], self.concat_labels[valid_index:]\n",
    "\n",
    "        train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "        valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
    "        test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "\n",
    "        train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "        valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
    "        test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "\n",
    "        train_dataloader = utils.DataLoader(train_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        valid_dataloader = utils.DataLoader(valid_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        test_dataloader = utils.DataLoader(test_dataset, batch_size=self.batch, shuffle=False, drop_last=True)\n",
    "        \n",
    "        return train_dataloader, valid_dataloader, test_dataloader, train_i, valid_i, test_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
